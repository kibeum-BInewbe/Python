{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO49ldyqEDimURsTLppxQ08"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"english_pos_tagging_2023.ipynb\n","\n","Automatically generated by Colaboratory.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1Spqrz04La2NGHh8pQiujKHVADaU4CqA0\n","\"\"\"\n","\n","## Englsish POS tagger developed for 2023-fall AI course.\n","## 개발의 시작은  단어/품사 문자열의 문장들을 가지는 다음 3 개의 파일들이다.\n","##   1) tagged corpus \"all_word_pos_sentences_all.txt\" :  Penn-tree-bank 전체의 모든 문장들을 가짐\n","##          . 이 파일은 단어 사진을 만드는데 이용함.\n","##\n","##   2) 위 파일을 3 개의 부분으로 미리 나누어 놓은 다음 파일들을  훈련, 검증, 테스트에 이용한다:\n","##          all_word_pos_sentences_train.txt , all_word_pos_sentences_all.txt\n","##          all_word_pos_sentences_test.txt\n","\n","import os\n","import time\n","import tensorflow as tf\n","from tensorflow.keras.layers import Embedding\n","from tensorflow.keras import layers\n","from tensorflow import keras\n","import numpy as np\n","from google.colab import drive\n","\n","## 상수 선언\n","d_embed = 100     ## word embedding vector 의 길이 (원소수)\n","Max_seq_length = 128    ## time step 수 (MSL)\n","Vocab_size = 51459\t# 사전 Vocab 의 총 단어수\n","Num_POS = 50    ## 총 품사 수\n","LEARNING_RATE = 0.7e-4\n","EPOCHS = 20\n","BATCH_SIZE = 64\n","\n","drive.mount('/content/drive')\n","\n","# 단어와 그 출현횟수를 dict 타입의 사전에 수집한다.\n","def build_vocabulary_temp(corpus_path):\n","    fp = open(corpus_path, \"r\", encoding=\"utf-8\")\n","    for line in fp.readlines():\n","        sentence = line.split()\n","        if sentence[0] == '<<':\n","            continue\n","        for word_pos_pair in sentence:\n","            w_p = word_pos_pair.split('/')\n","            nseg = len(w_p)\n","            ## word/pos 내에 슬래시가 2개 이상 있어 3 조각 이상이 나옴.\n","            if nseg > 2:    ## 마지막 슬래시를 기준으로 단어와 품사로 구분함.\n","                word = ''\n","                for i in range(nseg-1):\n","                    word = word + w_p[i] + '/'\n","                word = word[:-1]    # remove the last slash.\n","            else:\n","                word = w_p[0]\n","\n","            pos = w_p[-1]   # the last segment\n","            if word in Vocab_temporary:\n","                Vocab_temporary[word] += 1\n","            else:\n","                Vocab_temporary[word] = 1\n","    fp.close()\n","    return\n","\n","# 1 차적인 임시사전을 만든다. 사전명: Vocab_temporary\n","#    key: 단어, value:출현횟수\n","# 수집할 단어들의 대상은 penn-tree-bank 내의 모든 파일에 존재하는 단어들이다!!\n","\n","Vocab_temporary = {}\n","build_vocabulary_temp(\"./drive/MyDrive/all_word_pos_sentences_all.txt\")\n","\n","# 사전 단어들을 출현횟수로 내림차순으로 정렬하여 그 결과를 리스트로 받는다.\n","sorted_Vocab = sorted(Vocab_temporary.items(), key = lambda kv: kv[1], reverse=True)\n","Total_n_words = len(sorted_Vocab)\n","print('파일에서 모은 총 단어수:', Total_n_words)\n","\n","# 두 개의 특수단어('[PAD]', '[UNK]')를 포함하는 정식 사전(사전명: Vocab) 을 만든다:\n","#     key: 단어, value: 단어번호\n","Vocab = {}\n","Vocab['[PAD]'] = 0  # 특수단어 추가\n","Vocab['[UNK]'] = 1  # 특수단어 추가\n","# Vocab_temporary 에 모은 단어들에게는 단어번호를 2 부터 준다.\n","for i in range(Total_n_words):\n","    word = sorted_Vocab[i][0]\n","    freq = sorted_Vocab[i][1]\n","    Vocab[word] = i + 2\n","\n","Total_number_words = len(Vocab)\n","print(\"최종 사전의 총 단어수:\", Total_number_words)\n","\n","## 역-단어사전 i_Vocab  만들기:  Vocab 에서 key와 value 를 바꾼 사전.\n","#    key: 단어번호,  value: 단어\n","all_wps = list(Vocab.keys())\n","i_Vocab = {}\n","for word in all_wps:\n","  widx = Vocab[word]\n","  i_Vocab[widx] = word\n","\n","## 품사 사전 (사전명: dic_POS)만들기:  key:품사명,  value: 품사번호\n","#    두 특수단어에게는 품사명/품사번호를  '[PAD]': 0,     '[UNK]': 1 로 준다.\n","#    나머지는 penn-tree-bank 의 48 개의 품사들에게 품사 번호를 2 부터 부여한다(2~49)\n","#    결국 총 품사는 총 50 개로 번호는 0 ~ 49 가 된다.\n","all_pos_list = ['[PAD]', '[UNK]',\n","                'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS',\n","                'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB',\n","                'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN',\n","                'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '#', '$', '.', ',',\n","                ':', '(', ')', '\\'\\'', '\\'', '``', '&rsquo', '”']\n","dic_POS = {}\n","for i in range(len(all_pos_list)):\n","    dic_POS[all_pos_list[i]] = i\n","\n","num_pos = len(dic_POS)\n","print(\"총 품사수: \", num_pos)\n","\n","## 역-품사사전 만들기: dic_POS 에서 key와 value 를 바꾼 사전.\n","#     key: 품사번호,     value: 품사명\n","all_pos = list(dic_POS.keys())\n","i_dic_POS = {}\n","for a_pos in all_pos:\n","  pidx = dic_POS[a_pos]\n","  i_dic_POS[pidx] = a_pos\n","\n","def build_index_sentences(path_word_pos_sentence_file, path_index_sentence_file):\n","    fp = open(path_word_pos_sentence_file, \"r\", encoding=\"utf-8\")\n","    fp_w = open(path_index_sentence_file, \"w\", encoding=\"utf-8\")\n","\n","    for line in fp.readlines():\n","        sentence = line.split()\n","        if sentence[0] == '<<':   ## 파일명을 가지는 줄은 무시한다.\n","            continue\n","\n","        line_widx = ''  # line for word indices\n","        line_pidx = ''  # line for pos indices\n","\n","        for word_pos_pair in sentence:\n","            w_p = word_pos_pair.split('/')\n","            nseg = len(w_p)\n","            if nseg > 2:\n","                word = ''\n","                for i in range(nseg-1):\n","                    word = word + w_p[i] + '/'\n","                word = word[:-1]    # remove the last slash.\n","            else:\n","                word = w_p[0]\n","\n","            pos = w_p[-1]   # the last segment\n","            if not(word in Vocab):  # Other scheme: Vocab.get(word) 가 None 이면 없는 것을 맗암.\n","                widx = 1    # give index of [UNK] since it is missing in Vocab.\n","            else:\n","                widx = Vocab[word]\n","\n","            if not(pos in dic_POS):\n","                pos_list = pos.split('|')\n","                pos = pos_list[-1]\n","                if not (pos in dic_POS):\n","                    print(\"exception occurs at dic_POS look_up. w_p=\", w_p, \" pos=\", pos)\n","                    time.sleep(100)\n","                else:\n","                    pidx = dic_POS[pos]\n","            else:\n","                pidx = dic_POS[pos]\n","\n","            if len(line_widx) == 0:\n","                line_widx = line_widx + str(widx)\n","            else:\n","                line_widx = line_widx + '\\t' + str(widx)\n","\n","            if len(line_pidx) == 0:\n","                line_pidx = line_pidx + str(pidx)\n","            else:\n","                line_pidx = line_pidx + '\\t' + str(pidx)\n","\n","        fp_w.write(line_widx + '\\n')\n","        fp_w.write(line_pidx + '\\n')\n","        fp_w.write('\\n')    # an empty line after each sentence\n","    fp_w.close()\n","    fp.close()\n","\n","\n","## 단어/품사 문자열을 이용하는 훈련데이타 파일들로 부터  단어번호/품사번호를 이용하는 훈련데이타 파일들을 만든다.\n","build_index_sentences(\"./drive/MyDrive/all_word_pos_sentences_train.txt\", \"./drive/MyDrive/all_index_sentences_train.txt\")\n","build_index_sentences(\"./drive/MyDrive/all_word_pos_sentences_validation.txt\", \"./drive/MyDrive/all_index_sentences_validation.txt\")\n","build_index_sentences(\"./drive/MyDrive/all_word_pos_sentences_test.txt\", \"./drive/MyDrive/all_index_sentences_test.txt\")\n","\n","\n","##  훈련예제 준비 함수\n","##   파일경로를 입력으로 받는다. 이 파일은 all_index_sentences_???.txt 이다.\n","##  출력: 파일내의 모든 문장들에 대한 정보를 이용하여 다음을 준비하여 출력한다.\n","##       1) list_X: 문장들의 단어 번호 리스트를 원소로 가지는 리스트\n","##       2) list_Y: 문장들의 정답품사번호 리스트를 원소로 가지는 리스트\n","##       3) list_leng: 문장들의 길이를 원소로 가지는 리스트\n","\n","def load_X_and_Y(path_index_file):\n","    fp= open(path_index_file, \"r\", encoding=\"utf-8\")\n","    list_X = []\n","    list_Y = []\n","    list_leng = []\n","\n","    while True:\n","        # read two lines\n","        wordline = fp.readline()\n","        line_leng = len(wordline)\n","\n","        if line_leng == 0:\n","            break   # end of file has come.\n","        if line_leng == 1:\n","            continue    # empty line used as sentence delimeter\n","\n","        # The line read just before is a line of word indices.\n","        # The next line should be the corresponding pos index line.\n","        posline = fp.readline()\n","        w_index = wordline.split()\n","        p_index = posline.split()\n","\n","        # X : a list of indices of words in a sentence.\n","        # Y : a list of pos indices of words in the sentence of X.\n","        X = []\n","        Y = []\n","\n","        leng = len(w_index)\n","        if leng > Max_seq_length:\n","            leng = Max_seq_length   # truncation is done.\n","\n","        for i in range(leng):\n","            X.append(int(w_index[i]))\n","            Y.append(int(p_index[i]))\n","\n","        # pads are added after sentence\n","        if leng < Max_seq_length:\n","            for i in range(leng, Max_seq_length):\n","                X.append(0)     # word index of '[PAD]' which is 0 is added.\n","                Y.append(0)     # pos index of  '[PAD]' which is 0 is added.\n","\n","        list_X.append(X)\n","        list_Y.append(Y)\n","        list_leng.append(leng)\n","\n","    fp.close()\n","    return list_X, list_Y, list_leng\n","\n","x_train, y_train, leng_train = load_X_and_Y(\"./drive/MyDrive/all_index_sentences_train.txt\")\n","x_train = np.array(x_train, dtype='i')\n","y_train = np.array(y_train, dtype='i')\n","\n","x_validation, y_validation, leng_valiation = load_X_and_Y(\"./drive/MyDrive/all_index_sentences_validation.txt\")\n","x_validation = np.array(x_validation, dtype='i')\n","y_validation = np.array(y_validation, dtype='i')\n","\n","x_test, y_test, leng_test = load_X_and_Y(\"./drive/MyDrive/all_index_sentences_test.txt\")\n","x_test = np.array(x_test, dtype='i')\n","y_test = np.array(y_test, dtype='i')\n","\n","# Model 설계\n","\n","model = tf.keras.models.Sequential()\n","\n","## 층0:  word-embedding 층\n","model.add(tf.keras.layers.Embedding(Vocab_size, d_embed, embeddings_initializer='random_normal', \\\n","\tinput_length=Max_seq_length, mask_zero=True, trainable=True))\t# output shape: (bsz, MSL, d_emb)\n","\n","## 충1: LSTM 층\n","model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True), input_shape=(Max_seq_length, d_embed)))\n","\t# output shape: (batch_sz, MSL, 512)\n","\n","## 충2: LSTM 층\n","model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True), input_shape=(Max_seq_length, 512)))\n","\t# output shape: (batch_sz, MSL, 256)\n","\n","## 충3: LSTM 층\n","model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True), input_shape=(Max_seq_length, 256)))\n","\t# output shape: (batch_sz, MSL, 128)\n","\n","## 층4: NN 층\n","model.add(tf.keras.layers.Dense(units=Num_POS, activation='softmax', use_bias=True))\t# 최종출력층. 각 시간의 각 단어마다 num_POS=50개의 확률이 생성됨.\n","\t# output shape: (batch_sz, MSL, num_POS)\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n","model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=['acc'])\n","model.fit(x=x_train, y=y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(x_validation, y_validation), shuffle=True, verbose=1)\n","\n","print(\"...\")\n","\n","\n","# ...\n","\n","# ...\n","\n","##### TEST ####################\n","\n","pred = model.predict(x=x_test[:20], verbose=1)   # 첫 20개 예제에 대한 결과만 가져오도록 수정\n","pred_label = tf.math.argmax(pred, axis=2)\n","pred_label = pred_label.numpy()     # this is of 2-dimension (20, msl)\n","\n","num_test_sentences = 20  # 첫 20개 예제에 대해서만 처리\n","\n","# accuracy 계산\n","correct_count = 0\n","total_count = 0\n","\n","# 각 예제에 대해 단어별로 품사를 비교하여 정확도 계산\n","for i in range(num_test_sentences):\n","    for j in range(leng_test[i]):\n","        total_count += 1\n","        if pred_label[i, j] == y_test[i, j]:\n","            correct_count += 1\n","\n","# 정확도 계산\n","accuracy = correct_count / total_count\n","print(\"Test Accuracy: {:.4f}\".format(accuracy))\n","\n","# 품사인식 결과 출력 (첫 20개 예제에 대해서만)\n","for i in range(num_test_sentences):\n","    print(\"({})\".format(i+1), end=\" \")\n","    for j in range(leng_test[i]):\n","        word_idx = x_test[i, j]\n","        pos_idx_pred = pred_label[i, j]\n","        pos_idx_true = y_test[i, j]\n","\n","        # 역사전을 사용하여 단어와 품사를 출력\n","        word = i_Vocab[word_idx]\n","        pos_pred = i_dic_POS[pos_idx_pred]\n","        pos_true = i_dic_POS[pos_idx_true]\n","\n","        # 예측이 올바른 경우\n","        if pos_idx_pred == pos_idx_true:\n","            print(\"{}{}/{} \".format(word, pos_pred, pos_true), end=\"\")\n","        else:\n","            # 예측이 틀린 경우 정답 품사를 괄호에 넣어서 표시\n","            print(\"{}{}/<{}> \".format(word, pos_pred, pos_true), end=\"\")\n","    print()\n","\n","print(\"Program ends.\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AthvssgZdc7a","executionInfo":{"status":"ok","timestamp":1700120970275,"user_tz":-540,"elapsed":964746,"user":{"displayName":"권기범","userId":"11945951459495919079"}},"outputId":"e14c893f-694c-4016-8de3-6562cd3d2f3d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","파일에서 모은 총 단어수: 51457\n","최종 사전의 총 단어수: 51459\n","총 품사수:  50\n","Epoch 1/20\n","844/844 [==============================] - 116s 102ms/step - loss: 2.9218 - acc: 0.1806 - val_loss: 2.7987 - val_acc: 0.2015\n","Epoch 2/20\n","844/844 [==============================] - 46s 54ms/step - loss: 2.7293 - acc: 0.2150 - val_loss: 2.6086 - val_acc: 0.2886\n","Epoch 3/20\n","844/844 [==============================] - 43s 51ms/step - loss: 2.1379 - acc: 0.4362 - val_loss: 1.6512 - val_acc: 0.5514\n","Epoch 4/20\n","844/844 [==============================] - 41s 49ms/step - loss: 1.2383 - acc: 0.6792 - val_loss: 0.9388 - val_acc: 0.7796\n","Epoch 5/20\n","844/844 [==============================] - 48s 57ms/step - loss: 0.6872 - acc: 0.8507 - val_loss: 0.5528 - val_acc: 0.8819\n","Epoch 6/20\n","844/844 [==============================] - 40s 48ms/step - loss: 0.4110 - acc: 0.9140 - val_loss: 0.3917 - val_acc: 0.9112\n","Epoch 7/20\n","844/844 [==============================] - 42s 50ms/step - loss: 0.2876 - acc: 0.9370 - val_loss: 0.3161 - val_acc: 0.9248\n","Epoch 8/20\n","844/844 [==============================] - 41s 48ms/step - loss: 0.2210 - acc: 0.9486 - val_loss: 0.2754 - val_acc: 0.9319\n","Epoch 9/20\n","844/844 [==============================] - 43s 51ms/step - loss: 0.1802 - acc: 0.9563 - val_loss: 0.2497 - val_acc: 0.9366\n","Epoch 10/20\n","844/844 [==============================] - 45s 53ms/step - loss: 0.1527 - acc: 0.9617 - val_loss: 0.2338 - val_acc: 0.9408\n","Epoch 11/20\n","844/844 [==============================] - 40s 48ms/step - loss: 0.1330 - acc: 0.9656 - val_loss: 0.2221 - val_acc: 0.9437\n","Epoch 12/20\n","844/844 [==============================] - 43s 50ms/step - loss: 0.1184 - acc: 0.9684 - val_loss: 0.2158 - val_acc: 0.9449\n","Epoch 13/20\n","844/844 [==============================] - 41s 49ms/step - loss: 0.1074 - acc: 0.9703 - val_loss: 0.2098 - val_acc: 0.9459\n","Epoch 14/20\n","844/844 [==============================] - 44s 52ms/step - loss: 0.0988 - acc: 0.9721 - val_loss: 0.2053 - val_acc: 0.9469\n","Epoch 15/20\n","844/844 [==============================] - 42s 50ms/step - loss: 0.0918 - acc: 0.9733 - val_loss: 0.2034 - val_acc: 0.9474\n","Epoch 16/20\n","844/844 [==============================] - 40s 48ms/step - loss: 0.0862 - acc: 0.9743 - val_loss: 0.2013 - val_acc: 0.9482\n","Epoch 17/20\n","844/844 [==============================] - 44s 52ms/step - loss: 0.0816 - acc: 0.9755 - val_loss: 0.2014 - val_acc: 0.9484\n","Epoch 18/20\n","844/844 [==============================] - 40s 48ms/step - loss: 0.0777 - acc: 0.9764 - val_loss: 0.1983 - val_acc: 0.9492\n","Epoch 19/20\n","844/844 [==============================] - 43s 51ms/step - loss: 0.0743 - acc: 0.9772 - val_loss: 0.2001 - val_acc: 0.9491\n","Epoch 20/20\n","844/844 [==============================] - 40s 48ms/step - loss: 0.0715 - acc: 0.9778 - val_loss: 0.2011 - val_acc: 0.9492\n","...\n","1/1 [==============================] - 7s 7s/step\n","Test Accuracy: 0.9399\n","(1) ArthurNNP/NNP M.NNP/NNP GoldbergNNP/NNP saidVBD/VBD hePRP/PRP extendedVBD/VBD hisPRP$/PRP$ unsolicitedJJ/JJ tenderNN/NN offerNN/NN ofIN/IN $$/$ 32CD/CD \n","(2) aDT/DT shareNN/NN tenderNN/NN offerNN/NN ,,/, orCC/CC $$/$ 154.3CD/CD millionCD/CD ,,/, forIN/IN DiNNP/NNP GiorgioNNP/NNP Corp.NNP/NNP toTO/TO Nov.NNP/NNP 1CD/CD ../. \n","(3) DIGNNP/NNP AcquisitionNNP/NNP Corp.NNP/NNP ,,/, theDT/DT NewNNP/NNP JerseyNNP/NNP investorNN/NN 'sPOS/POS acquisitionNN/NN vehicleNN/NN ,,/, saidVBD/VBD thatIN/IN asIN/IN ofIN/IN theDT/DT closeNN/NN ofIN/IN businessNN/NN yesterdayNN/NN ,,/, 560,839NNP/<CD> sharesNNS/NNS hadVBD/VBD beenVBN/VBN tenderedVBN/VBN ../. \n","(4) IncludingVBG/VBG theDT/DT stakeNN/NN DIGNNP/NNP alreadyRB/RB heldVBD/VBD ,,/, DIGNNP/NNP holdsVBZ/VBZ aDT/DT totalNN/NN ofIN/IN aboutRB/RB 25CD/CD %NN/NN ofIN/IN DiNNP/NNP GiorgioNNP/NNP 'sPOS/POS sharesNNS/NNS onIN/IN aDT/DT fullyRB/RB dilutedVBN/VBN basisNN/NN ../. \n","(5) TheDT/DT offerNN/NN ,,/, whichWDT/WDT alsoRB/RB includesVBZ/VBZ commonJJ/JJ andCC/CC preferredJJ/<VBN> stockNN/NN purchaseNN/NN rightsNNS/NNS ,,/, wasVBD/VBD toTO/TO expireVB/VB lastJJ/JJ nightNN/NN atIN/IN midnightNN/NN ../. \n","(6) TheDT/DT newJJ/JJ expirationNN/NN dateNN/NN isVBZ/VBZ theDT/DT dateNN/NN onIN/IN whichWDT/WDT \n","(7) DIGNNP/NNP 'sPOS/POS financingNN/<VBG> commitmentsNNS/NNS ,,/, whichWDT/WDT totalNN/<VBP> aboutIN/<RB> $$/$ 240CD/CD millionCD/CD ,,/, areVBP/VBP toTO/TO expireVB/VB ../. \n","(8) DIGNNP/NNP isVBZ/VBZ aDT/DT unitNN/NN ofIN/IN DIGNNP/NNP HoldingNNP/NNP Corp.NNP/NNP ,,/, aDT/DT unitNN/NN ofIN/IN RoseNNP/NNP PartnersNNP/NNP L.PNNP/NNP ../. \n","(9) Mr.NNP/NNP GoldbergNNP/NNP isVBZ/VBZ theDT/DT soleJJ/JJ generalJJ/JJ partnerNN/NN inIN/IN RoseNNP/NNP PartnersNNP/NNP ../. \n","(10) InIN/IN AugustNNP/NNP ,,/, DiNNP/NNP GiorgioNNP/NNP ,,/, aDT/DT SanNNP/NNP FranciscoNNP/NNP foodNN/NN productsNNS/NNS andCC/CC buildingNN/NN materialsNNS/NNS marketingNN/NN andCC/CC distributionNN/NN companyNN/NN ,,/, rejectedVBD/VBD Mr.NNP/NNP GoldbergNNP/NNP 'sPOS/POS offerNN/NN asIN/IN inadequateJJ/JJ ../. \n","(11) InIN/IN NewNNP/NNP YorkNNP/NNP StockNNP/NNP ExchangeNNP/NNP compositeJJ/<NN> tradingNN/NN yesterdayNN/NN ,,/, DiNNP/NNP GiorgioNNP/NNP closedVBD/VBD atIN/IN $$/$ 31.50CD/CD \n","(12) aDT/DT shareNN/NN ,,/, downRB/RB $$/$ 1.75CD/CD ../. \n","(13) WhatWP/WP doesVBZ/VBZ n'tRB/RB belongVB/VB hereRB/RB ?./. \n","(14) A.NN/<LS> manualNNP/<JJ> typewritersNNS/NNS ,,/, \n","(15) B.NNP/<LS> black-and-whiteNNP/<JJ> snapshotsNNS/NNS ,,/, \n","(16) C.NNP/<LS> radioNN/NN adventureNN/NN showsVBZ/<NNS> ../. \n","(17) IfIN/IN youPRP/PRP guessedVBD/VBD black-and-whiteJJ/JJ snapshotsNNS/NNS ,,/, youPRP/PRP 'reVBP/VBP rightJJ/JJ ../. \n","(18) AfterIN/IN yearsNNS/NNS ofIN/IN fadingJJ/<NN> intoIN/IN theDT/DT backgroundNN/NN ,,/, two-toneVBG/<JJ> photographyNN/NN isVBZ/VBZ comingVBG/VBG backRB/RB ../. \n","(19) TrendyVBG/<JJ> magazineNN/NN advertisementsNNS/NNS featureVBP/VBP starkJJ/JJ black-and-whiteJJ/JJ photosNNS/NNS ofIN/IN HollywoodNNP/NNP celebritiesNNS/NNS pitchingVBG/VBG jeansNNS/NNS ,,/, shoesNNS/NNS andCC/CC liquorNN/NN ../. \n","(20) PortraitNN/NN studiosNNS/NNS accustomedVBD/<VBN> toTO/TO shootingVBG/VBG onlyRB/RB inIN/IN colorNN/NN reportNN/<VBP> aDT/DT rushNN/NN toTO/TO black-and-whiteJJ/JJ portraitNN/NN ordersNNS/NNS ../. \n","Program ends.\n"]}]}]}