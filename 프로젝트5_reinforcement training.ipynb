{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJF4tEn2EoS5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc735eae-78a0-44b8-b17a-1b869c20780b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Q table is\n",
            "\n",
            "col=0       1        2        3       4         5\n",
            "row: 0\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 1\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 2\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 3\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 4\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 5\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "\n",
            "Learning starts.\n",
            "\n",
            "episode= 0   epsilon= 0.4\n",
            "episode= 5000   epsilon= 0.36924653855465434\n",
            "episode= 10000   epsilon= 0.3408575155864846\n",
            "episode= 15000   epsilon= 0.3146511444266214\n",
            "episode= 20000   epsilon= 0.2904596148294764\n",
            "episode= 25000   epsilon= 0.26812801841425576\n",
            "episode= 30000   epsilon= 0.24751335672245633\n",
            "episode= 35000   epsilon= 0.22848362553952595\n",
            "episode= 40000   epsilon= 0.21091696961721942\n",
            "episode= 45000   epsilon= 0.19470090238398868\n",
            "\n",
            "Learning is finished. the Q table is:\n",
            "\n",
            "col=0       1        2        3       4         5\n",
            "row: 0\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 1\n",
            " 0.00,   -9.00,   -9.00,   -9.00,   -9.00,    0.00\n",
            " 0.00,    5.31,    5.90,    5.31,   -9.00,    0.00\n",
            " 0.00,    5.31,   -9.00,    6.56,   -9.00,    0.00\n",
            " 0.00,   -9.00,    4.78,    5.31,    5.90,    0.00\n",
            "row: 2\n",
            " 0.00,    4.78,    0.00,    5.90,    0.00,    0.00\n",
            " 0.00,   -9.00,    0.00,   -9.00,    0.00,    0.00\n",
            " 0.00,    5.90,    0.00,    7.29,    0.00,    0.00\n",
            " 0.00,   -9.00,    0.00,   -9.00,    0.00,    0.00\n",
            "row: 3\n",
            " 0.00,    5.31,   -9.00,    6.56,    0.00,    0.00\n",
            " 0.00,    6.56,    7.29,   -9.00,    0.00,    0.00\n",
            " 0.00,   -9.00,    7.29,    8.10,    0.00,    0.00\n",
            " 0.00,   -9.00,    5.90,    6.56,    0.00,    0.00\n",
            "row: 4\n",
            " 0.00,    0.00,    6.56,    7.29,    0.00,    0.00\n",
            " 0.00,    0.00,    8.10,    9.00,    0.00,    0.00\n",
            " 0.00,    0.00,   -9.00,   -9.00,    0.00,    0.00\n",
            " 0.00,    0.00,   -9.00,    7.29,    0.00,    0.00\n",
            "row: 5\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00,    0.00\n",
            "\n",
            "Test starts.\n",
            "\n",
            "\n",
            "Episode: 0  start state: ( 1 ,  1 )\n",
            "The move is: right  that leads to state ( 1 ,  2 )\n",
            "The move is: right  that leads to state ( 1 ,  3 )\n",
            "The move is: down   that leads to state ( 2 ,  3 )\n",
            "The move is: down   that leads to state ( 3 ,  3 )\n",
            "The move is: down   that leads to state ( 4 ,  3 )\n",
            "The move is: right  that leads to state ( 4 ,  4 )\n",
            "Episode has ended. Total reward received in episode =  9\n",
            "\n",
            "Episode: 1  start state: ( 1 ,  1 )\n",
            "The move is: right  that leads to state ( 1 ,  2 )\n",
            "The move is: right  that leads to state ( 1 ,  3 )\n",
            "The move is: down   that leads to state ( 2 ,  3 )\n",
            "The move is: down   that leads to state ( 3 ,  3 )\n",
            "The move is: down   that leads to state ( 4 ,  3 )\n",
            "The move is: right  that leads to state ( 4 ,  4 )\n",
            "Episode has ended. Total reward received in episode =  9\n",
            "\n",
            "Episode: 2  start state: ( 1 ,  1 )\n",
            "The move is: right  that leads to state ( 1 ,  2 )\n",
            "The move is: right  that leads to state ( 1 ,  3 )\n",
            "The move is: down   that leads to state ( 2 ,  3 )\n",
            "The move is: down   that leads to state ( 3 ,  3 )\n",
            "The move is: down   that leads to state ( 4 ,  3 )\n",
            "The move is: right  that leads to state ( 4 ,  4 )\n",
            "Episode has ended. Total reward received in episode =  9\n",
            "\n",
            "Episode: 3  start state: ( 1 ,  1 )\n",
            "The move is: right  that leads to state ( 1 ,  2 )\n",
            "The move is: right  that leads to state ( 1 ,  3 )\n",
            "The move is: down   that leads to state ( 2 ,  3 )\n",
            "The move is: down   that leads to state ( 3 ,  3 )\n",
            "The move is: down   that leads to state ( 4 ,  3 )\n",
            "The move is: right  that leads to state ( 4 ,  4 )\n",
            "Episode has ended. Total reward received in episode =  9\n",
            "\n",
            "Episode: 4  start state: ( 1 ,  1 )\n",
            "The move is: right  that leads to state ( 1 ,  2 )\n",
            "The move is: right  that leads to state ( 1 ,  3 )\n",
            "The move is: down   that leads to state ( 2 ,  3 )\n",
            "The move is: down   that leads to state ( 3 ,  3 )\n",
            "The move is: down   that leads to state ( 4 ,  3 )\n",
            "The move is: right  that leads to state ( 4 ,  4 )\n",
            "Episode has ended. Total reward received in episode =  9\n",
            "Program ends!!!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "import math\n",
        "from datetime import datetime\n",
        "\n",
        "total_episodes = 50000       # Total episodes\n",
        "max_steps = 99               # Max steps per episode\n",
        "gamma = 0.9                 # Discounting rate\n",
        "alpha = 1.0\t\t\t\t\t # update parameter\n",
        "\n",
        "# Exploration parameters\n",
        "original_epsilon = 0.4           # Exploration rate\n",
        "decay_rate = 0.000016            # Exponential decay rate for exploration prob\n",
        "random.seed(datetime.now().timestamp())\t# give a new seed in random number generation.\n",
        "\n",
        "# state space is defined as size_row X size_col array.\n",
        "# The boundary cells are holes(H).\n",
        "# S: start, G: goal, H:hole, F:frozen\n",
        "\n",
        "max_row = 6\n",
        "max_col = 6\n",
        "max_num_actions = 4\n",
        "\n",
        "env_state_space = \\\n",
        "  [ ['H', 'H', 'H', 'H', 'H', 'H'], \\\n",
        "    ['H', 'S', 'F', 'F', 'F', 'H'], \\\n",
        "\t\t['H', 'F', 'H', 'F', 'H', 'H'], \\\n",
        "\t\t['H', 'F', 'F', 'F', 'H', 'H'], \\\n",
        "\t\t['H', 'H', 'F', 'F', 'G', 'H'], \\\n",
        "\t\t['H', 'H', 'H', 'H', 'H', 'H'] ]\n",
        "\n",
        "# Create our Q table and initialize its value.\n",
        "#   dim0:row, dim1:column, dim2: action.\n",
        "# Q-table is initialized as 0.0.\n",
        "# for terminal states(H or G), q-a value should be always 0.\n",
        "\n",
        "Q = np.zeros((max_row,\tmax_col,  max_num_actions))\n",
        "\n",
        "# offset of each move action:  up, right, down, left, respectively.\n",
        "# a new state(location) = current state + offset of an action.\n",
        "#        action = up    right  down    left.\n",
        "move_offset = [[-1,0], [0,1],   [1,0],  [0,-1]]\n",
        "move_str =\t   ['up   ', 'right', 'down ', 'left ']\n",
        "\n",
        "def display_Q_table (Q):\n",
        "\tprint(\"\\ncol=0       1        2        3       4         5\")\n",
        "\tfor r in range(max_row):\n",
        "\t\tprint(\"row:\", r)\n",
        "\t\tfor a in range(max_num_actions):\n",
        "\t\t\tfor c in range(max_col):\n",
        "\t\t\t\ttext = \"{:5.2f}\".format(Q[r,c,a])\n",
        "\t\t\t\tif c == 0:\n",
        "\t\t\t\t\tline = text\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tline = line + \",   \" + text\n",
        "\t\t\tprint(line)\n",
        "\n",
        "# choose an action with epsilon-greedy approach according to Q.\n",
        "# return value: an action index(0 ~ 3)\n",
        "def choose_action_with_epsilon_greedy(s, epsilon):\n",
        "\tr = s[0]\n",
        "\tc = s[1]\n",
        "\t# get q-a values of all actions of current state.\n",
        "\tq_a_list = Q[r,c,:]\n",
        "\tmax = np.argmax(q_a_list)\t# max is action with biggest q-a value.\n",
        "\trn = random.random()\t# 0~1 사이의 random 실수 생성.\n",
        "\n",
        "\tif rn >= epsilon:\t# epsilon 보다 크면, action max is selected.\n",
        "\t\taction = max\n",
        "\telse:\n",
        "\t\trn1 = random.random()\n",
        "\t\t# 4 개의 action 중 하나를 무작위로 선택.\n",
        "\t\tif rn1 >= 0.75:\n",
        "\t\t\taction = 0\n",
        "\t\telif rn1>= 0.5:\n",
        "\t\t\taction = 1\n",
        "\t\telif rn1 >= 0.25:\n",
        "\t\t\taction = 2\n",
        "\t\telse:\n",
        "\t\t\taction = 3\n",
        "\treturn action\n",
        "\n",
        "# Q 가 가진 policy 를 greedy 하게 적용하여 취할 action 을 고른다.\n",
        "def choose_action_with_greedy(s):\n",
        "\tr = s[0]\n",
        "\tc = s[1]\n",
        "\t# get q-a values of all actions of state s.\n",
        "\tq_a_list = Q[r,c,:]\n",
        "\tmax_action = np.argmax(q_a_list)\t# max is action with biggest q-a value.\n",
        "\treturn max_action\n",
        "\n",
        "# get new state and reward for taking action a at state s.\n",
        "# deterministic movement is taken.\n",
        "# reward is given as: F/S:0;  H:-5;   G:5.\n",
        "def get_new_state_and_reward(s, a):\n",
        "\tnew_state = []\n",
        "\toff_set = move_offset[a]\n",
        "\n",
        "\t#  s + off_set gives the new_state.\n",
        "\tnew_state.append(s[0] + off_set[0])\n",
        "\tnew_state.append(s[1] + off_set[1])\n",
        "\n",
        "\t# compute reward for moving to the new state\n",
        "\tcell = env_state_space[new_state[0]][new_state[1]]\n",
        "\tif cell == 'F':\n",
        "\t\trew = 0\n",
        "\telif cell == 'H':\n",
        "\t\trew = -9\n",
        "\telif cell == 'G':\n",
        "\t\trew = 9\n",
        "\telif cell == 'S':\n",
        "\t\trew = 0\n",
        "\telse:\n",
        "\t\tprint(\"Logic error in get_new_state_and_reward. This cannot happen!\")\n",
        "\t\ttime.sleep(1200)\n",
        "\t\treturn [0,0], -20000\n",
        "\treturn new_state, rew\n",
        "\n",
        "# Environment 출력: agent 가 있는 곳에는 * 로 표시.\n",
        "# agent 의 현재 위치(즉 current state): s\n",
        "def env_rendering(s):\n",
        "\tfor i in range(0, max_row, 1):\n",
        "\t\tline = ''\n",
        "\t\tfor j in range(0, max_col, 1):\n",
        "\t\t\tline = line + env_state_space[i][j]\n",
        "\t\tif s[0] == i:\n",
        "\t\t\tcol = s[1]\n",
        "\t\t\tline1 = line[:col] + '*' +line[col+1:]\n",
        "\t\telse:\n",
        "\t\t\tline1 = line\n",
        "\t\tprint(line1)\n",
        "\n",
        "# Learning stage: it iterates for an huge number of episodes\n",
        "print(\"Initial Q table is\")\n",
        "display_Q_table(Q)\n",
        "\n",
        "# start state is the cell with 'S'. terminal states are those with 'H' or 'G.\n",
        "start_state = [1,1]\n",
        "\n",
        "print(\"\\nLearning starts.\\n\")\n",
        "for episode in range(total_episodes):\n",
        "\t# set the start state of an episode.\n",
        "\tS = start_state\n",
        "\n",
        "\t# we use decayed epsilon in exploration so that it decreases as time goes on.\n",
        "\tepsilon = original_epsilon * math.exp(-decay_rate*episode)\n",
        "\n",
        "\tif episode % 5000 == 0:\n",
        "\t\tprint('episode=', episode, '  epsilon=', epsilon)\n",
        "\t\ttime.sleep(1)\n",
        "\n",
        "\tfor step in range(max_steps):\n",
        "\t\t# Choose an action A from S using policy derived from Q with epsilon-greedy.\n",
        "\t\tA = choose_action_with_epsilon_greedy(S, epsilon)\n",
        "\n",
        "\t\t# take action A to observe reward R, and new state S_.\n",
        "\t\tS_ , R = get_new_state_and_reward(S, A)\n",
        "\n",
        "\t\tr=S[0]\t# row of S\n",
        "\t\tc=S[1]\t# column of S\n",
        "\n",
        "\t\t# update state-action value Q(s,a)\n",
        "\t\tQ[r][c][A] = Q[r][c][A] + alpha * (R + gamma * np.max(Q[S_[0]][S_[1]][:]) - Q[r][c][A] )\n",
        "\n",
        "\t\tS = S_\t# move to the new state.\n",
        "\n",
        "\t\t# if the new state S is a terminal state, terminate the episode.\n",
        "\t\tif env_state_space[S[0]][S[1]] == 'G' or env_state_space[S[0]][S[1]] == 'H':\n",
        "\t\t\tbreak\n",
        "\n",
        "print('\\nLearning is finished. the Q table is:')\n",
        "display_Q_table(Q)\n",
        "# time.sleep(600)\n",
        "\n",
        "# Test stage: agent 가 길을 찾아 가는 실험.\n",
        "\n",
        "print(\"\\nTest starts.\\n\")\n",
        "\n",
        "for e in range(5):\n",
        "\tS = start_state\n",
        "\ttotal_rewards = 0\n",
        "\tprint(\"\\nEpisode:\", e, \" start state: (\", S[0], \", \", S[1], \")\")\n",
        "\n",
        "\tfor step in range(max_steps):\n",
        "\n",
        "\t\tA = choose_action_with_greedy(S)\t\t# S 에서 greedy 하게 다음 action 을 선택.\n",
        "\n",
        "\t\tS_ , R = get_new_state_and_reward(S, A)\n",
        "\n",
        "\t\tprint(\"The move is:\", move_str[A], \" that leads to state (\", S_[0], \", \", S_[1], \")\" )\n",
        "\n",
        "\t\ttotal_rewards += R\n",
        "\t\tS = S_\n",
        "\t\tif env_state_space[S[0]][S[1]] == 'G' or env_state_space[S[0]][S[1]] == 'H':\n",
        "\n",
        "\t\t\tbreak\n",
        "\tprint(\"Episode has ended. Total reward received in episode = \", total_rewards)\n",
        "\ttime.sleep(1)\n",
        "\n",
        "print(\"Program ends!!!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "import math\n",
        "from datetime import datetime\n",
        "\n",
        "total_episodes = 50000       # Total episodes\n",
        "max_steps = 99               # Max steps per episode\n",
        "gamma = 0.9                 # Discounting rate\n",
        "alpha = 1.0                 # update parameter\n",
        "\n",
        "# Exploration parameters\n",
        "original_epsilon = 0.4           # Exploration rate\n",
        "decay_rate = 0.000016            # Exponential decay rate for exploration prob\n",
        "random.seed(datetime.now().timestamp())  # give a new seed in random number generation.\n",
        "\n",
        "# state space is defined as size_row X size_col array.\n",
        "# The boundary cells are holes(H).\n",
        "# S: start, G: goal, H:hole, F:frozen\n",
        "\n",
        "max_row = 5\n",
        "max_col = 5\n",
        "max_num_actions = 4\n",
        "\n",
        "env_state_space = [\n",
        "    ['H', 'H', 'H', 'H', 'H'],\n",
        "    ['H', 'S', 'F', 'F', 'H'],\n",
        "    ['H', 'F', 'H', 'F', 'H'],\n",
        "    ['H', 'F', 'F', 'F', 'H'],\n",
        "    ['H', 'H', 'F', 'F', 'G']\n",
        "]\n",
        "\n",
        "# Create our Q table and initialize its value.\n",
        "#   dim0:row, dim1:column, dim2: action.\n",
        "# Q-table is initialized as 0.0.\n",
        "# for terminal states(H or G), q-a value should be always 0.\n",
        "\n",
        "Q = np.zeros((max_row, max_col, max_num_actions))\n",
        "\n",
        "# offset of each move action:  up, right, down, left, respectively.\n",
        "# a new state(location) = current state + offset of an action.\n",
        "#        action = up    right  down    left.\n",
        "move_offset = [[-1, 0], [0, 1], [1, 0], [0, -1]]\n",
        "move_str = ['up   ', 'right', 'down ', 'left ']\n",
        "\n",
        "\n",
        "def display_Q_table(Q):\n",
        "    print(\"\\ncol=0       1        2        3       4\")\n",
        "    for r in range(max_row):\n",
        "        print(\"row:\", r)\n",
        "        for a in range(max_num_actions):\n",
        "            for c in range(max_col):\n",
        "                text = \"{:5.2f}\".format(Q[r, c, a])\n",
        "                if c == 0:\n",
        "                    line = text\n",
        "                else:\n",
        "                    line = line + \",   \" + text\n",
        "            print(line)\n",
        "\n",
        "\n",
        "# choose an action with epsilon-greedy approach according to Q.\n",
        "# return value: an action index(0 ~ 3)\n",
        "def my_argmax(q_a_list):\n",
        "    max_value = np.max(q_a_list)\n",
        "    m_positions = np.where(q_a_list == max_value)[0]\n",
        "    nmax = len(m_positions)\n",
        "    thresh = np.arange(1, nmax + 1) / nmax\n",
        "    r_n = random.random()\n",
        "\n",
        "    for i in range(nmax):\n",
        "        if r_n <= thresh[i]:\n",
        "            return m_positions[i]\n",
        "\n",
        "\n",
        "def choose_action_with_epsilon_greedy(s, epsilon):\n",
        "    r = s[0]\n",
        "    c = s[1]\n",
        "    # get q-a values of all actions of current state.\n",
        "    q_a_list = Q[r, c, :]\n",
        "    max_action = my_argmax(q_a_list)  # use custom argmax function\n",
        "    rn = random.random()  # 0~1 사이의 random 실수 생성.\n",
        "\n",
        "    if rn >= epsilon:  # epsilon 보다 크면, action max is selected.\n",
        "        action = max_action\n",
        "    else:\n",
        "        rn1 = random.random()\n",
        "        # 4 개의 action 중 하나를 무작위로 선택.\n",
        "        if rn1 >= 0.75:\n",
        "            action = 0\n",
        "        elif rn1 >= 0.5:\n",
        "            action = 1\n",
        "        elif rn1 >= 0.25:\n",
        "            action = 2\n",
        "        else:\n",
        "            action = 3\n",
        "    return action\n",
        "\n",
        "\n",
        "# Q 가 가진 policy 를 greedy 하게 적용하여 취할 action 을 고른다.\n",
        "def choose_action_with_greedy(s):\n",
        "    r = s[0]\n",
        "    c = s[1]\n",
        "    # get q-a values of all actions of state s.\n",
        "    q_a_list = Q[r, c, :]\n",
        "    max_action = my_argmax(q_a_list)  # use custom argmax function\n",
        "    return max_action\n",
        "\n",
        "\n",
        "# get new state and reward for taking action a at state s.\n",
        "# deterministic movement is taken.\n",
        "# reward is given as: F/S:0;  H:-5;   G:5.\n",
        "def get_new_state_and_reward(s, a):\n",
        "    new_state = []\n",
        "    off_set = move_offset[a]\n",
        "\n",
        "    # s + off_set gives the new_state.\n",
        "    new_state.append(s[0] + off_set[0])\n",
        "    new_state.append(s[1] + off_set[1])\n",
        "\n",
        "    # Check if the new state is within the valid range\n",
        "    if 0 <= new_state[0] < max_row and 0 <= new_state[1] < max_col:\n",
        "        # compute reward for moving to the new state\n",
        "        cell = env_state_space[new_state[0]][new_state[1]]\n",
        "        if cell == 'F':\n",
        "            rew = 0\n",
        "        elif cell == 'H':\n",
        "            rew = -9\n",
        "        elif cell == 'G':\n",
        "            rew = 9\n",
        "        elif cell == 'S':\n",
        "            rew = 0\n",
        "        else:\n",
        "            print(\"Logic error in get_new_state_and_reward. This cannot happen!\")\n",
        "            time.sleep(1200)\n",
        "            return [0, 0], -20000\n",
        "        return new_state, rew\n",
        "    else:\n",
        "        # If the new state is outside the valid range, return the same state and a penalty reward\n",
        "        return s, -9\n",
        "\n",
        "\n",
        "# Environment 출력: agent 가 있는 곳에는 * 로 표시.\n",
        "# agent 의 현재 위치(즉 current state): s\n",
        "def env_rendering(s):\n",
        "    for i in range(0, max_row, 1):\n",
        "        line = ''\n",
        "        for j in range(0, max_col, 1):\n",
        "            line = line + env_state_space[i][j]\n",
        "        if s[0] == i:\n",
        "            col = s[1]\n",
        "            line1 = line[:col] + '*' + line[col + 1:]\n",
        "        else:\n",
        "            line1 = line\n",
        "        print(line1)\n",
        "\n",
        "\n",
        "# Learning stage: it iterates for an huge number of episodes\n",
        "print(\"Initial Q table is\")\n",
        "display_Q_table(Q)\n",
        "\n",
        "# start state is the cell with 'S'. terminal states are those with 'H' or 'G'.\n",
        "start_state = [1, 1]\n",
        "\n",
        "print(\"\\nLearning starts.\\n\")\n",
        "for episode in range(total_episodes):\n",
        "    # set the start state of an episode.\n",
        "    S = start_state\n",
        "\n",
        "    # we use decayed epsilon in exploration so that it decreases as time goes on.\n",
        "    epsilon = original_epsilon * math.exp(-decay_rate * episode)\n",
        "\n",
        "    if episode % 5000 == 0:\n",
        "        print('episode=', episode, '  epsilon=', epsilon)\n",
        "        time.sleep(1)\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        # Choose an action A from S using policy derived from Q with epsilon-greedy.\n",
        "        A = choose_action_with_epsilon_greedy(S, epsilon)\n",
        "\n",
        "        # take action A to observe reward R, and new state S_.\n",
        "        S_, R = get_new_state_and_reward(S, A)\n",
        "\n",
        "        r = S[0]  # row of S\n",
        "        c = S[1]  # column of S\n",
        "\n",
        "        # update state-action value Q(s,a)\n",
        "        Q[r][c][A] = Q[r][c][A] + alpha * (R + gamma * np.max(Q[S_[0]][S_[1]][:]) - Q[r][c][A])\n",
        "\n",
        "        S = S_  # move to the new state.\n",
        "\n",
        "        # if the new state S is a terminal state, terminate the episode.\n",
        "        if env_state_space[S[0]][S[1]] == 'G' or env_state_space[S[0]][S[1]] == 'H':\n",
        "            break\n",
        "\n",
        "print('\\nLearning is finished. the Q table is:')\n",
        "display_Q_table(Q)\n",
        "\n",
        "# Test stage: agent 가 길을 찾아 가는 실험.\n",
        "\n",
        "print(\"\\nTest starts.\\n\")\n",
        "\n",
        "for e in range(5):\n",
        "    S = start_state\n",
        "    total_rewards = 0\n",
        "    print(\"\\nEpisode:\", e, \" start state: (\", S[0], \", \", S[1], \")\")\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        A = choose_action_with_greedy(S)  # S 에서 greedy 하게 다음 action 을 선택.\n",
        "\n",
        "        S_, R = get_new_state_and_reward(S, A)\n",
        "\n",
        "        print(\"The move is:\", move_str[A], \" that leads to state (\", S_[0], \", \", S_[1], \")\")\n",
        "\n",
        "        total_rewards += R\n",
        "        S = S_\n",
        "        if env_state_space[S[0]][S[1]] == 'G' or env_state_space[S[0]][S[1]] == 'H':\n",
        "            break\n",
        "    print(\"Episode has ended. Total reward received in episode = \", total_rewards)\n",
        "    time.sleep(1)\n",
        "\n",
        "print(\"Program ends!!!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmCMe5HV0KNz",
        "outputId": "e00910e3-ba63-4001-f151-8b80cd00198f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Q table is\n",
            "\n",
            "col=0       1        2        3       4\n",
            "row: 0\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 1\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 2\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 3\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 4\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            "\n",
            "Learning starts.\n",
            "\n",
            "episode= 0   epsilon= 0.4\n",
            "episode= 5000   epsilon= 0.36924653855465434\n",
            "episode= 10000   epsilon= 0.3408575155864846\n",
            "episode= 15000   epsilon= 0.3146511444266214\n",
            "episode= 20000   epsilon= 0.2904596148294764\n",
            "episode= 25000   epsilon= 0.26812801841425576\n",
            "episode= 30000   epsilon= 0.24751335672245633\n",
            "episode= 35000   epsilon= 0.22848362553952595\n",
            "episode= 40000   epsilon= 0.21091696961721942\n",
            "episode= 45000   epsilon= 0.19470090238398868\n",
            "\n",
            "Learning is finished. the Q table is:\n",
            "\n",
            "col=0       1        2        3       4\n",
            "row: 0\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 1\n",
            " 0.00,   -9.00,   -9.00,   -9.00,    0.00\n",
            " 0.00,    5.31,    5.90,   -9.00,    0.00\n",
            " 0.00,    5.31,   -9.00,    6.56,    0.00\n",
            " 0.00,   -9.00,    4.78,    5.31,    0.00\n",
            "row: 2\n",
            " 0.00,    4.78,    0.00,    5.90,    0.00\n",
            " 0.00,   -9.00,    0.00,   -9.00,    0.00\n",
            " 0.00,    5.90,    0.00,    7.29,    0.00\n",
            " 0.00,   -9.00,    0.00,   -9.00,    0.00\n",
            "row: 3\n",
            " 0.00,    5.31,   -9.00,    6.56,    0.00\n",
            " 0.00,    6.56,    7.29,   -9.00,    0.00\n",
            " 0.00,   -9.00,    7.29,    8.10,    0.00\n",
            " 0.00,   -9.00,    5.90,    6.56,    0.00\n",
            "row: 4\n",
            " 0.00,    0.00,    6.56,    7.29,    0.00\n",
            " 0.00,    0.00,    8.10,    9.00,    0.00\n",
            " 0.00,    0.00,   -1.71,   -0.90,    0.00\n",
            " 0.00,    0.00,   -9.00,    7.29,    0.00\n",
            "\n",
            "Test starts.\n",
            "\n",
            "\n",
            "Episode: 0  start state: ( 1 ,  1 )\n",
            "The move is: right  that leads to state ( 1 ,  2 )\n",
            "The move is: right  that leads to state ( 1 ,  3 )\n",
            "The move is: down   that leads to state ( 2 ,  3 )\n",
            "The move is: down   that leads to state ( 3 ,  3 )\n",
            "The move is: down   that leads to state ( 4 ,  3 )\n",
            "The move is: right  that leads to state ( 4 ,  4 )\n",
            "Episode has ended. Total reward received in episode =  9\n",
            "\n",
            "Episode: 1  start state: ( 1 ,  1 )\n",
            "The move is: right  that leads to state ( 1 ,  2 )\n",
            "The move is: right  that leads to state ( 1 ,  3 )\n",
            "The move is: down   that leads to state ( 2 ,  3 )\n",
            "The move is: down   that leads to state ( 3 ,  3 )\n",
            "The move is: down   that leads to state ( 4 ,  3 )\n",
            "The move is: right  that leads to state ( 4 ,  4 )\n",
            "Episode has ended. Total reward received in episode =  9\n",
            "\n",
            "Episode: 2  start state: ( 1 ,  1 )\n",
            "The move is: down   that leads to state ( 2 ,  1 )\n",
            "The move is: down   that leads to state ( 3 ,  1 )\n",
            "The move is: right  that leads to state ( 3 ,  2 )\n",
            "The move is: down   that leads to state ( 4 ,  2 )\n",
            "The move is: right  that leads to state ( 4 ,  3 )\n",
            "The move is: right  that leads to state ( 4 ,  4 )\n",
            "Episode has ended. Total reward received in episode =  9\n",
            "\n",
            "Episode: 3  start state: ( 1 ,  1 )\n",
            "The move is: right  that leads to state ( 1 ,  2 )\n",
            "The move is: right  that leads to state ( 1 ,  3 )\n",
            "The move is: down   that leads to state ( 2 ,  3 )\n",
            "The move is: down   that leads to state ( 3 ,  3 )\n",
            "The move is: down   that leads to state ( 4 ,  3 )\n",
            "The move is: right  that leads to state ( 4 ,  4 )\n",
            "Episode has ended. Total reward received in episode =  9\n",
            "\n",
            "Episode: 4  start state: ( 1 ,  1 )\n",
            "The move is: right  that leads to state ( 1 ,  2 )\n",
            "The move is: right  that leads to state ( 1 ,  3 )\n",
            "The move is: down   that leads to state ( 2 ,  3 )\n",
            "The move is: down   that leads to state ( 3 ,  3 )\n",
            "The move is: down   that leads to state ( 4 ,  3 )\n",
            "The move is: right  that leads to state ( 4 ,  4 )\n",
            "Episode has ended. Total reward received in episode =  9\n",
            "Program ends!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "import math\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "\n",
        "total_episodes = 50000       # Total episodes\n",
        "max_steps = 99               # Max steps per episode\n",
        "gamma = 0.9                 # Discounting rate\n",
        "alpha = 1.0                 # Update parameter\n",
        "\n",
        "# Exploration parameters\n",
        "original_epsilon = 0.4           # Exploration rate\n",
        "decay_rate = 0.000016            # Exponential decay rate for exploration probability\n",
        "random.seed(datetime.now().timestamp())  # Give a new seed in random number generation.\n",
        "\n",
        "# State space is defined as a size_row X size_col array.\n",
        "# The boundary cells are holes(H).\n",
        "# S: start, G: goal, H: hole, F: frozen\n",
        "\n",
        "max_row = 5\n",
        "max_col = 5\n",
        "max_num_actions = 4\n",
        "\n",
        "env_state_space = [\n",
        "    ['H', 'H', 'H', 'H', 'H'],\n",
        "    ['H', 'S', 'F', 'F', 'H'],\n",
        "    ['H', 'F', 'H', 'F', 'H'],\n",
        "    ['H', 'F', 'F', 'F', 'H'],\n",
        "    ['H', 'H', 'F', 'F', 'G']\n",
        "]\n",
        "\n",
        "# Create our Q table and initialize its value.\n",
        "#   dim0: row, dim1: column, dim2: action.\n",
        "# Q-table is initialized as 0.0.\n",
        "# For terminal states (H or G), Q-value should be always 0.\n",
        "Q = np.zeros((max_row, max_col, max_num_actions))\n",
        "\n",
        "# Offset of each move action:  up, right, down, left, respectively.\n",
        "# A new state (location) = current state + offset of an action.\n",
        "#        action = up    right  down    left.\n",
        "move_offset = [[-1, 0], [0, 1], [1, 0], [0, -1]]\n",
        "move_str = ['up   ', 'right', 'down ', 'left ']\n",
        "\n",
        "# 1. Neural network model design\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(max_row, max_col)),\n",
        "    tf.keras.layers.Flatten(),  # Flatten the matrix to 1D\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(max_num_actions)  # Output nodes represent possible actions\n",
        "])\n",
        "\n",
        "# 2. Replay Buffer setup\n",
        "replay_buffer = deque(maxlen=10000)\n",
        "\n",
        "# 3. Target Network setup\n",
        "target_model = tf.keras.models.clone_model(model)\n",
        "target_model.set_weights(model.get_weights())\n",
        "\n",
        "# 4. Learning algorithm modification\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# Batch size for replay buffer\n",
        "batch_size = 32\n",
        "\n",
        "# Interval for updating the target network\n",
        "target_update_interval = 1000\n",
        "\n",
        "def env_state_space_to_input(env_state_space, state):\n",
        "    # Convert the environment state space to the input format for the neural network\n",
        "    input_state = np.zeros((max_row, max_col))\n",
        "    input_state[state[0], state[1]] = 1.0  # Mark the current position with 1.0\n",
        "    for r in range(max_row):\n",
        "        for c in range(max_col):\n",
        "            if env_state_space[r][c] == 'H':\n",
        "                input_state[r, c] = -1.0  # Mark holes with -1.0\n",
        "            elif env_state_space[r][c] == 'F':\n",
        "                input_state[r, c] = 0.5  # Mark frozen cells with 0.5\n",
        "    return input_state\n",
        "\n",
        "def display_Q_table(Q):\n",
        "    print(\"\\ncol=0       1        2        3       4\")\n",
        "    for r in range(max_row):\n",
        "        print(\"row:\", r)\n",
        "        for a in range(max_num_actions):\n",
        "            for c in range(max_col):\n",
        "                text = \"{:5.2f}\".format(Q[r, c, a])\n",
        "                if c == 0:\n",
        "                    line = text\n",
        "                else:\n",
        "                    line = line + \",   \" + text\n",
        "            print(line)\n",
        "\n",
        "def choose_action_with_epsilon_greedy(s, epsilon):\n",
        "    r = s[0]\n",
        "    c = s[1]\n",
        "    q_a_list = Q[r, c, :]\n",
        "    max_action = np.argmax(q_a_list)\n",
        "\n",
        "    if random.random() < epsilon:\n",
        "        return random.randint(0, max_num_actions - 1)\n",
        "    else:\n",
        "        return max_action\n",
        "\n",
        "def get_new_state_and_reward(s, a):\n",
        "    new_state = []\n",
        "    off_set = move_offset[a]\n",
        "\n",
        "    # s + off_set gives the new_state.\n",
        "    new_state.append(s[0] + off_set[0])\n",
        "    new_state.append(s[1] + off_set[1])\n",
        "\n",
        "    # Check if the new state is within the valid range\n",
        "    if 0 <= new_state[0] < max_row and 0 <= new_state[1] < max_col:\n",
        "        # compute reward\n",
        "        cell = env_state_space[new_state[0]][new_state[1]]\n",
        "        if cell == 'F':\n",
        "            rew = 0\n",
        "        elif cell == 'H':\n",
        "            rew = -9\n",
        "        elif cell == 'G':\n",
        "            rew = 9\n",
        "        elif cell == 'S':\n",
        "            rew = 0\n",
        "        else:\n",
        "            print(\"Logic error in get_new_state_and_reward. This cannot happen!\")\n",
        "            return [0, 0], -20000\n",
        "        return new_state, rew\n",
        "    else:\n",
        "        # If the new state is outside the valid range, return the same state and a penalty reward\n",
        "        return s, -9\n",
        "\n",
        "def env_rendering(s):\n",
        "    for i in range(0, max_row, 1):\n",
        "        line = ''\n",
        "        for j in range(0, max_col, 1):\n",
        "            line = line + env_state_space[i][j]\n",
        "        if s[0] == i:\n",
        "            col = s[1]\n",
        "            line1 = line[:col] + '*' + line[col + 1:]\n",
        "        else:\n",
        "            line1 = line\n",
        "        print(line1)\n",
        "\n",
        "# Learning stage\n",
        "print(\"Initial Q table is\")\n",
        "display_Q_table(Q)\n",
        "\n",
        "start_state = [1, 1]\n",
        "\n",
        "print(\"\\nLearning starts.\\n\")\n",
        "\n",
        "for episode in range(total_episodes):\n",
        "    S = start_state\n",
        "    epsilon = original_epsilon * math.exp(-decay_rate * episode)\n",
        "\n",
        "    if episode % 5000 == 0:\n",
        "        print('episode=', episode, '  epsilon=', epsilon)\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        A = choose_action_with_epsilon_greedy(S, epsilon)\n",
        "        S_, R = get_new_state_and_reward(S, A)\n",
        "\n",
        "        r = S[0]\n",
        "        c = S[1]\n",
        "        Q[r][c][A] = Q[r][c][A] + alpha * (R + gamma * np.max(Q[S_[0]][S_[1]][:]) - Q[r][c][A])\n",
        "\n",
        "        S = S_\n",
        "\n",
        "        if env_state_space[S[0]][S[1]] == 'G' or env_state_space[S[0]][S[1]] == 'H':\n",
        "            break\n",
        "\n",
        "print('\\nLearning is finished. the Q table is:')\n",
        "display_Q_table(Q)\n",
        "\n",
        "# Test stage\n",
        "print(\"\\nTest starts.\\n\")\n",
        "\n",
        "for e in range(5):\n",
        "    S = start_state\n",
        "    total_rewards = 0\n",
        "    print(\"\\nEpisode:\", e, \" Start state: (\", S[0], \", \", S[1], \")\")\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        A = choose_action_with_epsilon_greedy(S, 0.0)\n",
        "        S_, R = get_new_state_and_reward(S, A)\n",
        "\n",
        "        r = S[0]\n",
        "        c = S[1]\n",
        "        total_rewards += R\n",
        "\n",
        "        env_rendering(S)\n",
        "\n",
        "        print(\"Step:\", step, \" S=\", S, \" A=\", move_str[A], \" R=\", R, \" S_=\", S_)\n",
        "\n",
        "        if env_state_space[S_[0]][S_[1]] == 'G' or env_state_space[S_[0]][S_[1]] == 'H':\n",
        "            break\n",
        "        S = S_\n",
        "\n",
        "    print(\"Episode has ended. Total reward received in episode =\", total_rewards)\n",
        "    time.sleep(1)\n",
        "\n",
        "print(\"Program ends!!!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuRTcNQpNWU8",
        "outputId": "7c192e2e-9c90-45e1-ca31-3322fa5b9fd4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Q table is\n",
            "\n",
            "col=0       1        2        3       4\n",
            "row: 0\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 1\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 2\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 3\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 4\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            "\n",
            "Learning starts.\n",
            "\n",
            "episode= 0   epsilon= 0.4\n",
            "episode= 5000   epsilon= 0.36924653855465434\n",
            "episode= 10000   epsilon= 0.3408575155864846\n",
            "episode= 15000   epsilon= 0.3146511444266214\n",
            "episode= 20000   epsilon= 0.2904596148294764\n",
            "episode= 25000   epsilon= 0.26812801841425576\n",
            "episode= 30000   epsilon= 0.24751335672245633\n",
            "episode= 35000   epsilon= 0.22848362553952595\n",
            "episode= 40000   epsilon= 0.21091696961721942\n",
            "episode= 45000   epsilon= 0.19470090238398868\n",
            "\n",
            "Learning is finished. the Q table is:\n",
            "\n",
            "col=0       1        2        3       4\n",
            "row: 0\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            " 0.00,    0.00,    0.00,    0.00,    0.00\n",
            "row: 1\n",
            " 0.00,   -9.00,   -9.00,   -9.00,    0.00\n",
            " 0.00,    5.31,    5.90,   -9.00,    0.00\n",
            " 0.00,    5.31,   -9.00,    6.56,    0.00\n",
            " 0.00,   -9.00,    4.78,    5.31,    0.00\n",
            "row: 2\n",
            " 0.00,    4.78,    0.00,    5.90,    0.00\n",
            " 0.00,   -9.00,    0.00,   -9.00,    0.00\n",
            " 0.00,    5.90,    0.00,    7.29,    0.00\n",
            " 0.00,   -9.00,    0.00,   -9.00,    0.00\n",
            "row: 3\n",
            " 0.00,    5.31,   -9.00,    6.56,    0.00\n",
            " 0.00,    6.56,    7.29,   -9.00,    0.00\n",
            " 0.00,   -9.00,    7.29,    8.10,    0.00\n",
            " 0.00,   -9.00,    5.90,    6.56,    0.00\n",
            "row: 4\n",
            " 0.00,    0.00,    6.56,    7.29,    0.00\n",
            " 0.00,    0.00,    8.10,    9.00,    0.00\n",
            " 0.00,    0.00,   -1.71,   -0.90,    0.00\n",
            " 0.00,    0.00,   -9.00,    7.29,    0.00\n",
            "\n",
            "Test starts.\n",
            "\n",
            "\n",
            "Episode: 0  Start state: ( 1 ,  1 )\n",
            "HHHHH\n",
            "H*FFH\n",
            "HFHFH\n",
            "HFFFH\n",
            "HHFFG\n",
            "Step: 0  S= [1, 1]  A= right  R= 0  S_= [1, 2]\n",
            "HHHHH\n",
            "HS*FH\n",
            "HFHFH\n",
            "HFFFH\n",
            "HHFFG\n",
            "Step: 1  S= [1, 2]  A= right  R= 0  S_= [1, 3]\n",
            "HHHHH\n",
            "HSF*H\n",
            "HFHFH\n",
            "HFFFH\n",
            "HHFFG\n",
            "Step: 2  S= [1, 3]  A= down   R= 0  S_= [2, 3]\n",
            "HHHHH\n",
            "HSFFH\n",
            "HFH*H\n",
            "HFFFH\n",
            "HHFFG\n",
            "Step: 3  S= [2, 3]  A= down   R= 0  S_= [3, 3]\n",
            "HHHHH\n",
            "HSFFH\n",
            "HFHFH\n",
            "HFF*H\n",
            "HHFFG\n",
            "Step: 4  S= [3, 3]  A= down   R= 0  S_= [4, 3]\n",
            "HHHHH\n",
            "HSFFH\n",
            "HFHFH\n",
            "HFFFH\n",
            "HHF*G\n",
            "Step: 5  S= [4, 3]  A= right  R= 9  S_= [4, 4]\n",
            "Episode has ended. Total reward received in episode = 9\n",
            "\n",
            "Episode: 1  Start state: ( 1 ,  1 )\n",
            "HHHHH\n",
            "H*FFH\n",
            "HFHFH\n",
            "HFFFH\n",
            "HHFFG\n",
            "Step: 0  S= [1, 1]  A= right  R= 0  S_= [1, 2]\n",
            "HHHHH\n",
            "HS*FH\n",
            "HFHFH\n",
            "HFFFH\n",
            "HHFFG\n",
            "Step: 1  S= [1, 2]  A= right  R= 0  S_= [1, 3]\n",
            "HHHHH\n",
            "HSF*H\n",
            "HFHFH\n",
            "HFFFH\n",
            "HHFFG\n",
            "Step: 2  S= [1, 3]  A= down   R= 0  S_= [2, 3]\n",
            "HHHHH\n",
            "HSFFH\n",
            "HFH*H\n",
            "HFFFH\n",
            "HHFFG\n",
            "Step: 3  S= [2, 3]  A= down   R= 0  S_= [3, 3]\n",
            "HHHHH\n",
            "HSFFH\n",
            "HFHFH\n",
            "HFF*H\n",
            "HHFFG\n",
            "Step: 4  S= [3, 3]  A= down   R= 0  S_= [4, 3]\n",
            "HHHHH\n",
            "HSFFH\n",
            "HFHFH\n",
            "HFFFH\n",
            "HHF*G\n",
            "Step: 5  S= [4, 3]  A= right  R= 9  S_= [4, 4]\n",
            "Episode has ended. Total reward received in episode = 9\n",
            "\n",
            "Episode: 2  Start state: ( 1 ,  1 )\n",
            "HHHHH\n",
            "H*FFH\n",
            "HFHFH\n",
            "HFFFH\n",
            "HHFFG\n",
            "Step: 0  S= [1, 1]  A= right  R= 0  S_= [1, 2]\n",
            "HHHHH\n",
            "HS*FH\n",
            "HFHFH\n",
            "HFFFH\n",
            "HHFFG\n",
            "Step: 1  S= [1, 2]  A= right  R= 0  S_= [1, 3]\n",
            "HHHHH\n",
            "HSF*H\n",
            "HFHFH\n",
            "HFFFH\n",
            "HHFFG\n",
            "Step: 2  S= [1, 3]  A= down   R= 0  S_= [2, 3]\n",
            "HHHHH\n",
            "HSFFH\n",
            "HFH*H\n",
            "HFFFH\n",
            "HHFFG\n",
            "Step: 3  S= [2, 3]  A= down   R= 0  S_= [3, 3]\n",
            "HHHHH\n",
            "HSFFH\n",
            "HFHFH\n",
            "HFF*H\n",
            "HHFFG\n",
            "Step: 4  S= [3, 3]  A= down   R= 0  S_= [4, 3]\n",
            "HHHHH\n",
            "HSFFH\n",
            "HFHFH\n",
            "HFFFH\n",
            "HHF*G\n",
            "Step: 5  S= [4, 3]  A= right  R= 9  S_= [4, 4]\n",
            "Episode has ended. Total reward received in episode = 9\n",
            "\n",
            "Episode: 3  Start state: ( 1 ,  1 )\n",
            "HHHHH\n",
            "H*FFH\n",
            "HFHFH\n",
            "HFFFH\n",
            "HHFFG\n",
            "Step: 0  S= [1, 1]  A= right  R= 0  S_= [1, 2]\n",
            "HHHHH\n",
            "HS*FH\n",
            "HFHFH\n",
            "HFFFH\n",
            "HHFFG\n",
            "Step: 1  S= [1, 2]  A= right  R= 0  S_= [1, 3]\n",
            "HHHHH\n",
            "HSF*H\n",
            "HFHFH\n",
            "HFFFH\n",
            "HHFFG\n",
            "Step: 2  S= [1, 3]  A= down   R= 0  S_= [2, 3]\n",
            "HHHHH\n",
            "HSFFH\n",
            "HFH*H\n",
            "HFFFH\n",
            "HHFFG\n",
            "Step: 3  S= [2, 3]  A= down   R= 0  S_= [3, 3]\n",
            "HHHHH\n",
            "HSFFH\n",
            "HFHFH\n",
            "HFF*H\n",
            "HHFFG\n",
            "Step: 4  S= [3, 3]  A= down   R= 0  S_= [4, 3]\n",
            "HHHHH\n",
            "HSFFH\n",
            "HFHFH\n",
            "HFFFH\n",
            "HHF*G\n",
            "Step: 5  S= [4, 3]  A= right  R= 9  S_= [4, 4]\n",
            "Episode has ended. Total reward received in episode = 9\n",
            "\n",
            "Episode: 4  Start state: ( 1 ,  1 )\n",
            "HHHHH\n",
            "H*FFH\n",
            "HFHFH\n",
            "HFFFH\n",
            "HHFFG\n",
            "Step: 0  S= [1, 1]  A= right  R= 0  S_= [1, 2]\n",
            "HHHHH\n",
            "HS*FH\n",
            "HFHFH\n",
            "HFFFH\n",
            "HHFFG\n",
            "Step: 1  S= [1, 2]  A= right  R= 0  S_= [1, 3]\n",
            "HHHHH\n",
            "HSF*H\n",
            "HFHFH\n",
            "HFFFH\n",
            "HHFFG\n",
            "Step: 2  S= [1, 3]  A= down   R= 0  S_= [2, 3]\n",
            "HHHHH\n",
            "HSFFH\n",
            "HFH*H\n",
            "HFFFH\n",
            "HHFFG\n",
            "Step: 3  S= [2, 3]  A= down   R= 0  S_= [3, 3]\n",
            "HHHHH\n",
            "HSFFH\n",
            "HFHFH\n",
            "HFF*H\n",
            "HHFFG\n",
            "Step: 4  S= [3, 3]  A= down   R= 0  S_= [4, 3]\n",
            "HHHHH\n",
            "HSFFH\n",
            "HFHFH\n",
            "HFFFH\n",
            "HHF*G\n",
            "Step: 5  S= [4, 3]  A= right  R= 9  S_= [4, 4]\n",
            "Episode has ended. Total reward received in episode = 9\n",
            "Program ends!!!\n"
          ]
        }
      ]
    }
  ]
}